{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS229 Problem Set4\n",
    "## Problem 6 强化学习：倒立摆\n",
    "\n",
    "`cart_pole.py`为我们所使用的仿真器，可以用于模仿动作的进行，返回cart的状态，展示当前的cart等。\n",
    "`control.py` 为我们所需要进行决策的代码，其中给出了一定的框架，我们仅需要填写部分标明的了核心代码。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**首先让我们导入所需要的库文件**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function  # 兼容性\n",
    "from cart_pole import CartPole, Physics\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.signal import lfilter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化\n",
    "### 仿真器变量\n",
    "为框架提供的代码，不需要自己填写，但可以根据自己的需求更改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pause_time = 0.0001\n",
    "min_trial_length_to_start_display = 100\n",
    "display_started = min_trial_length_to_start_display == 0\n",
    "\n",
    "NUM_STATES = 163\n",
    "NUM_ACTIONS = 2\n",
    "GAMMA = 0.995\n",
    "TOLERANCE = 0.01\n",
    "NO_LEARNING_THRESHOLD = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `pause_time` :控制展示时帧与帧之间的时间，值越大，播放的速度就越慢\n",
    "- `min_trial_length_to_start_display` :只有在成功平衡这么多次的实验后，才允许进行展示。设置其为一个合理的高值（约为100）可以允许你快死的进行学习，并在有了一定的性能之后进行展示。\n",
    "- `display_started` ： 控制是否开始展示，为1时进行展示\n",
    "\n",
    "- `NUM_STATES` ： 总共的离散状态数。所有的状态从0到`NUM_STAES-1`进行编号，最后一个状态`NUM_STAES-1`是一个特殊状态，表示着实验失败（pole倒下或cart出界）\n",
    "- `NUM_ACTIONS` : 仅有2个动作，0代表向右推，1代表向左拉\n",
    "- `GAMMA` ： 折扣因子`\\gamma`\n",
    "- `TOLERANCE` : 控制每一轮价值迭代的收敛标准，小于这个值代表收敛。\n",
    "- `NO_LEARNING_THRESHOLD` ：当我们有`NO_LEARNING_THRESHOLD`个连续的价值函数的计算都收敛了，我们可以假设我们的所有的计算都收敛了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 其他变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time = 0 # 循环的次数\n",
    "\n",
    "time_steps_to_failure = []  #记录第几次失败\n",
    "num_failures = 0            #记录失败了多少次\n",
    "time_at_start_of_current_trial = 0 #进行当前实验时是第几次\n",
    "\n",
    "max_failures = 500          # 在失败这么多次前还没收敛，就停止\n",
    "\n",
    "cart_pole = CartPole(Physics()) #初始化小车和极点\n",
    "\n",
    "x, x_dot, theta, theta_dot = 0.0, 0.0, 0.0, 0.0  #连续状态向量\n",
    "state_tuple = (x, x_dot, theta, theta_dot)\n",
    "state = cart_pole.get_state(state_tuple) # 离散的状态\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 填写初始化代码\n",
    "**要求：** \n",
    "1. 假设还没有观察到状态转移和反馈\n",
    "2. 随机初始化价值函数向量为小的随机数（0到0.10）\n",
    "3. 均匀地初始化状态转移概率\n",
    "4. 初始化所有反馈为0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### BEGIN YOUR CODE ######\n",
    "# TODO:\n",
    "value = np.random.random((NUM_STATES,1))* 0.1\n",
    "trans_probs = np.ones((NUM_STATES,NUM_STATES,NUM_ACTIONS))/NUM_STATES\n",
    "trans_counts = np.zeros((NUM_STATES,NUM_STATES,NUM_ACTIONS))\n",
    "reward_counts = np.zeros((NUM_STATES,NUM_ACTIONS))\n",
    "reward = np.zeros((NUM_STATES,1))\n",
    "\n",
    "#raise NotImplementedError('Initalizations not implemented')\n",
    "###### END YOUR CODE ######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 填写MDP模型训练代码\n",
    "**要求：**\n",
    "1. 随机选择动作（0或者1）\n",
    "2. 根据当前的价值函数选择最优策略\n",
    "即$\\pi^{*}(s)=\\arg \\max _{a \\in A} \\sum_{s^{\\prime} \\in S} P_{s a}\\left(s^{\\prime}\\right) V^{*}\\left(s^{\\prime}\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "consecutive_no_learning_trials = 0\n",
    "while consecutive_no_learning_trials < NO_LEARNING_THRESHOLD:\n",
    "       ###### BEGIN YOUR CODE ######\n",
    "    # TODO:\n",
    "    score1 = trans_probs[state,:,0].dot(value)  ## 计算当前状态的价值函数\n",
    "    score2 = trans_probs[state,:,1].dot(value)\n",
    "\n",
    "    if score1>score2:\n",
    "        action = 0\n",
    "    elif score1<score2:\n",
    "      action = 1\n",
    "    else:\n",
    "        action = 0 if np.random.uniform() < 0.5 else 1\n",
    "    # raise NotImplementedError('Action choice not implemented')\n",
    "    # action = 0 if np.random.uniform() < 0.5 else 1\n",
    "    ###### END YOUR CODE ######\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    state_tuple = cart_pole.simulate(action, state_tuple)\n",
    "    # x, x_dot, theta, theta_dot = state_tuple\n",
    "\n",
    "    # 随着每次的模拟而递增\n",
    "    time = time + 1\n",
    "\n",
    "    # 得到新的离散化的状态\n",
    "    new_state = cart_pole.get_state(state_tuple)\n",
    "    # if display_started == 1:\n",
    "    #     cart_pole.show_cart(state_tuple, pause_time)\n",
    "\n",
    "    # reward function to use - do not change this!\n",
    "    if new_state == NUM_STATES - 1:\n",
    "        R = -1\n",
    "    else:\n",
    "        R = 0\n",
    "        #R = -np.abs(state_tuple[theta])/2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 填写模型更新代码\n",
    "我们已经得到了从`state`到`new_state`使用的`action`，和新状态下的反馈\n",
    "**要求**：\n",
    "1. 记录`state,action,new_state`产生次数的编号\n",
    "2. 记录每一个`new_state`的反馈\n",
    "3. 记录达到`new_state`的次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "     ###### BEGIN YOUR CODE ######\n",
    "    # TODO:\n",
    "    trans_counts[state,new_state,action] += 1\n",
    "    reward_counts[new_state,0] += R\n",
    "    reward_counts[new_state,1] += 1\n",
    "    #raise NotImplementedError('Update T and R not implemented')\n",
    "    ###### END YOUR CODE ######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.更新MDP的转移概率和反馈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    if new_state == NUM_STATES - 1:\n",
    "        for a in range(NUM_ACTIONS):\n",
    "            for s in range(NUM_STATES):\n",
    "                den = np.sum(trans_counts[s,:,a])\n",
    "                if den>0:\n",
    "                    trans_probs[s,:,a] = trans_counts[s,:,a] / den\n",
    "        for s in range(NUM_STATES):\n",
    "            if reward_counts[s,1]>0:\n",
    "                reward[s] = reward_counts[s,0] / reward_counts[s,1]\n",
    "        #raise NotImplementedError('MDP  T and R update not implemented')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 填写价值迭代算法\n",
    "**要求：**\n",
    "1. 使用参数`TOLERANCE`来判断收敛\n",
    "2. 若在第一轮迭代中收敛，更新变量，并检测整个程序是否该截止"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "     ###### BEGIN YOUR CODE ######\n",
    "        # TODO:\n",
    "        iterations = 0\n",
    "        while True:\n",
    "            iterations += 1\n",
    "            new_value = np.zeros((NUM_STATES, 1))  #必须在内层循环进行初始化，否则将会导致value和new_value公用同一内存地址\n",
    "            for s in range(NUM_STATES):\n",
    "                t_values = []\n",
    "                for a in range(NUM_ACTIONS) :\n",
    "                    t_values.append(trans_probs[s,:,a].dot(value))\n",
    "                new_value[s] = np.max(t_values)\n",
    "            new_value = reward + GAMMA*new_value\n",
    "            diff = np.max(np.abs(value-new_value))\n",
    "            value = new_value\n",
    "            if diff<TOLERANCE:\n",
    "                break\n",
    "        if iterations==1:\n",
    "            consecutive_no_learning_trials +=1\n",
    "        else:\n",
    "            consecutive_no_learning_trials =0\n",
    "\n",
    "        #raise NotImplementedError('Value iteration choice not implemented')\n",
    "        ###### END YOUR CODE ######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 剩余框架代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # when the pole fell and the state must be reinitialized.\n",
    "    if new_state == NUM_STATES - 1:\n",
    "        num_failures += 1\n",
    "        if num_failures >= max_failures:\n",
    "            break\n",
    "        print('[INFO] Failure number {}'.format(num_failures))\n",
    "        time_steps_to_failure.append(time - time_at_start_of_current_trial)\n",
    "        # time_steps_to_failure[num_failures] = time - time_at_start_of_current_trial\n",
    "        time_at_start_of_current_trial = time\n",
    "\n",
    "        if time_steps_to_failure[num_failures - 1] > min_trial_length_to_start_display:\n",
    "            display_started = 1\n",
    "\n",
    "        # Reinitialize state\n",
    "        # x = 0.0\n",
    "        x = -1.1 + np.random.uniform() * 2.2\n",
    "        x_dot, theta, theta_dot = 0.0, 0.0, 0.0\n",
    "        state_tuple = (x, x_dot, theta, theta_dot)\n",
    "        state = cart_pole.get_state(state_tuple)\n",
    "    else:\n",
    "        state = new_state\n",
    "\n",
    "# plot the learning curve (time balanced vs. trial)\n",
    "log_tstf = np.log(np.array(time_steps_to_failure))\n",
    "plt.plot(np.arange(len(time_steps_to_failure)), log_tstf, 'k')\n",
    "window = 30\n",
    "w = np.array([1/window for _ in range(window)])\n",
    "weights = lfilter(w, 1, log_tstf)\n",
    "x = np.arange(window//2, len(log_tstf) - window//2)\n",
    "plt.plot(x, weights[window:len(log_tstf)], 'r--')\n",
    "plt.xlabel('Num failures')\n",
    "plt.ylabel('Num steps to failure')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
